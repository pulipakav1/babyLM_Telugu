{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdK1Jp5lDq1j",
        "outputId": "60e6e937-acfe-4c86-dd36-c12c0e6429b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 447213 sorted sentences.\n"
          ]
        }
      ],
      "source": [
        "# Load sorted_sentences from file\n",
        "with open('/content/curriculum_sorted.txt', 'r', encoding='utf-8') as f:\n",
        "    sorted_sentences = [line.strip() for line in f.readlines()]\n",
        "print(f\"Loaded {len(sorted_sentences)} sorted sentences.\")\n",
        "\n",
        "curriculum_sentences = sorted_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpH4eIyNCTS-",
        "outputId": "7bbd907f-7969-47c8-b35c-5cfc50e52e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Curriculum: 447213 sentences\n",
            "Random: 25793 sentences\n",
            "Child-directed: 100000 sentences\n",
            "New dataset sizes: Curriculum 500, Random 100, Child-directed 500\n",
            "Final Vocab size after filtering: 790\n",
            "Using device: cpu\n",
            "Training on Curriculum...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 2.4102\n",
            "Epoch 2, Loss: 2.6049\n",
            "Training on Random...\n",
            "Epoch 1, Loss: 4.2493\n",
            "Epoch 2, Loss: 4.1448\n",
            "Training on Child-directed...\n",
            "Epoch 1, Loss: 3.0251\n",
            "Epoch 2, Loss: 2.9251\n",
            "Evaluating...\n",
            "Curriculum Perplexity: 13.19\n",
            "Random Perplexity: 62.69\n",
            "Child-directed Perplexity: 17.75\n",
            "Minimal pair: Correct loss=7.2830, Wrong loss=7.9648\n",
            "Minimal pair: Correct loss=4.8156, Wrong loss=4.7088\n"
          ]
        }
      ],
      "source": [
        "# 02_telugu_babylm_training.ipynb\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Load datasets\n",
        "curriculum_sentences = sorted_sentences  # from previous notebook\n",
        "random_sentences = pd.read_csv('/content/telugu_books.csv')['text'].tolist()\n",
        "child_sentences = pd.read_csv('/content/child_directed_telugu_1Lakh.csv')['sentence'].tolist()\n",
        "\n",
        "print(f\"Curriculum: {len(curriculum_sentences)} sentences\")\n",
        "print(f\"Random: {len(random_sentences)} sentences\")\n",
        "print(f\"Child-directed: {len(child_sentences)} sentences\")\n",
        "\n",
        "# Basic tokenizer (whitespace split)\n",
        "# Basic tokenizer (whitespace split)\n",
        "def tokenize(sentence):\n",
        "    if not isinstance(sentence, str):\n",
        "        sentence = str(sentence)\n",
        "    return sentence.split()\n",
        "\n",
        "# Clean sentences properly\n",
        "def clean_sentences(sent_list):\n",
        "    clean = []\n",
        "    for s in sent_list:\n",
        "        if isinstance(s, str) and s.strip() != '':\n",
        "            clean.append(s)\n",
        "    return clean\n",
        "\n",
        "# Clean all datasets\n",
        "curriculum_sentences = clean_sentences(curriculum_sentences)\n",
        "random_sentences = clean_sentences(random_sentences)\n",
        "child_sentences = clean_sentences(child_sentences)\n",
        "\n",
        "# Now build vocab\n",
        "# New code (replace here)\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenize sentences better (split by Telugu words properly)\n",
        "def tokenize(sentence):\n",
        "    if not isinstance(sentence, str):\n",
        "        sentence = str(sentence)\n",
        "    return re.findall(r'\\w+', sentence)\n",
        "\n",
        "# Clean sentences first\n",
        "def clean_sentences(sent_list):\n",
        "    clean = []\n",
        "    for s in sent_list:\n",
        "        if isinstance(s, str) and s.strip() != '':\n",
        "            clean.append(s)\n",
        "    return clean\n",
        "\n",
        "# Clean all datasets\n",
        "# Subsample datasets for fast training (for testing)\n",
        "curriculum_sentences = curriculum_sentences[:500]\n",
        "random_sentences = random_sentences[:100]\n",
        "child_sentences = child_sentences[:500]\n",
        "\n",
        "print(f\"New dataset sizes: Curriculum {len(curriculum_sentences)}, Random {len(random_sentences)}, Child-directed {len(child_sentences)}\")\n",
        "\n",
        "\n",
        "# Build vocab smarter\n",
        "word_counter = Counter()\n",
        "for s in (curriculum_sentences + random_sentences + child_sentences):\n",
        "    word_counter.update(tokenize(s))\n",
        "\n",
        "# Keep only frequent words\n",
        "MIN_FREQ = 5\n",
        "vocab = [word for word, freq in word_counter.items() if freq >= MIN_FREQ]\n",
        "\n",
        "word2idx = {w: i+1 for i, w in enumerate(vocab)}  # 0 = padding\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "vocab_size = len(word2idx) + 1\n",
        "print(f\"Final Vocab size after filtering: {vocab_size}\")\n",
        "\n",
        "\n",
        "# Encode sentences\n",
        "def encode(sent):\n",
        "    return [word2idx[w] for w in tokenize(sent) if w in word2idx]\n",
        "\n",
        "# Model: simple Transformer\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, heads=4, layers=6):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=heads)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Train function\n",
        "def train_model(train_sentences, epochs=2):\n",
        "    model = SimpleTransformer(vocab_size).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        random.shuffle(train_sentences)\n",
        "        for sent in train_sentences:\n",
        "            ids = encode(sent)\n",
        "            if len(ids) < 2:\n",
        "                continue\n",
        "            input_ids = torch.tensor([ids[:-1]], device=device)\n",
        "            target_ids = torch.tensor([ids[1:]], device=device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_ids)\n",
        "            loss = criterion(output.view(-1, vocab_size), target_ids.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_sentences):.4f}\")\n",
        "    return model\n",
        "\n",
        "# Perplexity function\n",
        "def calculate_perplexity(model, sentences):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_count = 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
        "    with torch.no_grad():\n",
        "        for sent in sentences:\n",
        "            ids = encode(sent)\n",
        "            if len(ids) < 2:\n",
        "                continue\n",
        "            input_ids = torch.tensor([ids[:-1]], device=device)\n",
        "            target_ids = torch.tensor([ids[1:]], device=device)\n",
        "            output = model(input_ids)\n",
        "            loss = criterion(output.view(-1, vocab_size), target_ids.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_count += len(ids) - 1\n",
        "    ppl = math.exp(total_loss / total_count)\n",
        "    return ppl\n",
        "\n",
        "# Train three models\n",
        "print(\"Training on Curriculum...\")\n",
        "curr_model = train_model(curriculum_sentences)\n",
        "\n",
        "print(\"Training on Random...\")\n",
        "rand_model = train_model(random_sentences)\n",
        "\n",
        "print(\"Training on Child-directed...\")\n",
        "child_model = train_model(child_sentences)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Evaluating...\")\n",
        "curr_ppl = calculate_perplexity(curr_model, curriculum_sentences[:1000])\n",
        "rand_ppl = calculate_perplexity(rand_model, random_sentences[:1000])\n",
        "child_ppl = calculate_perplexity(child_model, child_sentences[:1000])\n",
        "\n",
        "print(f\"Curriculum Perplexity: {curr_ppl:.2f}\")\n",
        "print(f\"Random Perplexity: {rand_ppl:.2f}\")\n",
        "print(f\"Child-directed Perplexity: {child_ppl:.2f}\")\n",
        "\n",
        "# Minimal pair testing\n",
        "minimal_pairs = [\n",
        "    (\"అమ్మాయిలు బడికి వెళ్తున్నారు.\", \"అమ్మాయిలు బడికి వెళ్తుంది.\"),\n",
        "    (\"అతను పాలు తాగాడు.\", \"అతను పాలు తాగింది.\"),\n",
        "]\n",
        "\n",
        "for correct, wrong in minimal_pairs:\n",
        "    c_ids = encode(correct)\n",
        "    w_ids = encode(wrong)\n",
        "    if len(c_ids) < 2 or len(w_ids) < 2:\n",
        "        continue\n",
        "    input_c = torch.tensor([c_ids[:-1]], device=device)\n",
        "    target_c = torch.tensor([c_ids[1:]], device=device)\n",
        "    input_w = torch.tensor([w_ids[:-1]], device=device)\n",
        "    target_w = torch.tensor([w_ids[1:]], device=device)\n",
        "    c_loss = nn.CrossEntropyLoss(ignore_index=0)(curr_model(input_c).view(-1, vocab_size), target_c.view(-1))\n",
        "    w_loss = nn.CrossEntropyLoss(ignore_index=0)(curr_model(input_w).view(-1, vocab_size), target_w.view(-1))\n",
        "    print(f\"Minimal pair: Correct loss={c_loss.item():.4f}, Wrong loss={w_loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3Jm8b8qES9I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
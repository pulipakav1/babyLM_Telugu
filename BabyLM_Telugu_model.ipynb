{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpH4eIyNCTS-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdK1Jp5lDq1j",
        "outputId": "b6ffebd9-3c49-4b7e-9738-3dd189039bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 447213 sorted sentences.\n"
          ]
        }
      ],
      "source": [
        "# Load sorted_sentences from file\n",
        "with open('/content/curriculum_sorted.txt', 'r', encoding='utf-8') as f:\n",
        "    sorted_sentences = [line.strip() for line in f.readlines()]\n",
        "print(f\"Loaded {len(sorted_sentences)} sorted sentences.\")\n",
        "\n",
        "curriculum_sentences = sorted_sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curriculum_sentences = sorted_sentences  # from previous notebook\n",
        "random_sentences = pd.read_csv('/content/telugu_books.csv')['text'].tolist()\n",
        "child_sentences = pd.read_csv('/content/child_directed_telugu_1Lakh.csv')['sentence'].tolist()\n"
      ],
      "metadata": {
        "id": "bQZRmLOR25VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3Jm8b8qES9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f2fa21-e551-4d92-8d58-32db78d78b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Curriculum: 447213 sentences\n",
            "Random: 25793 sentences\n",
            "Child-directed: 100000 sentences\n"
          ]
        }
      ],
      "source": [
        "print(f\"Curriculum: {len(curriculum_sentences)} sentences\")\n",
        "print(f\"Random: {len(random_sentences)} sentences\")\n",
        "print(f\"Child-directed: {len(child_sentences)} sentences\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    if not isinstance(sentence, str):\n",
        "        sentence = str(sentence)\n",
        "    return sentence.split()\n",
        "\n",
        "# Clean sentences properly\n",
        "def clean_sentences(sent_list):\n",
        "    clean = []\n",
        "    for s in sent_list:\n",
        "        if isinstance(s, str) and s.strip() != '':\n",
        "            clean.append(s)\n",
        "    return clean"
      ],
      "metadata": {
        "id": "SjNgwHfP29Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curriculum_sentences = clean_sentences(curriculum_sentences)\n",
        "random_sentences = clean_sentences(random_sentences)\n",
        "child_sentences = clean_sentences(child_sentences)"
      ],
      "metadata": {
        "id": "bEhxex4v3GN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    if not isinstance(sentence, str):\n",
        "        sentence = str(sentence)\n",
        "    return re.findall(r'\\w+', sentence)"
      ],
      "metadata": {
        "id": "ODjhASYL3LNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sentences(sent_list):\n",
        "    clean = []\n",
        "    for s in sent_list:\n",
        "        if isinstance(s, str) and s.strip() != '':\n",
        "            clean.append(s)\n",
        "    return clean"
      ],
      "metadata": {
        "id": "zt5JKvpD3MPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curriculum_sentences = curriculum_sentences[:500]\n",
        "random_sentences = random_sentences[:100]\n",
        "child_sentences = child_sentences[:500]"
      ],
      "metadata": {
        "id": "urGfgCAP3QGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"New dataset sizes: Curriculum {len(curriculum_sentences)}, Random {len(random_sentences)}, Child-directed {len(child_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XcjD_423P6i",
        "outputId": "a1f645ac-5722-4215-87c5-1a096b90e616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New dataset sizes: Curriculum 500, Random 100, Child-directed 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_counter = Counter()\n",
        "for s in (curriculum_sentences + random_sentences + child_sentences):\n",
        "    word_counter.update(tokenize(s))"
      ],
      "metadata": {
        "id": "mdk-RC-13W7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_FREQ = 5\n",
        "vocab = [word for word, freq in word_counter.items() if freq >= MIN_FREQ]\n",
        "word2idx = {w: i+1 for i, w in enumerate(vocab)}  # 0 = padding\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "vocab_size = len(word2idx) + 1\n",
        "print(f\"Final Vocab size after filtering: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4a7HBQL3aQA",
        "outputId": "e437bec2-0b71-49f8-93f3-2dbb6b08060c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Vocab size after filtering: 790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(sent):\n",
        "    return [word2idx[w] for w in tokenize(sent) if w in word2idx]"
      ],
      "metadata": {
        "id": "FIhTZttC3dt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, heads=4, layers=6):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=heads)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U_uletW3g05",
        "outputId": "fdaee652-08bb-4d3b-c0f6-8eb1467e3619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_sentences, epochs=2):\n",
        "    model = SimpleTransformer(vocab_size).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        random.shuffle(train_sentences)\n",
        "        for sent in train_sentences:\n",
        "            ids = encode(sent)\n",
        "            if len(ids) < 2:\n",
        "                continue\n",
        "            input_ids = torch.tensor([ids[:-1]], device=device)\n",
        "            target_ids = torch.tensor([ids[1:]], device=device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_ids)\n",
        "            loss = criterion(output.view(-1, vocab_size), target_ids.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_sentences):.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "PwufMvZl3lSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, sentences):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_count = 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
        "    with torch.no_grad():\n",
        "        for sent in sentences:\n",
        "            ids = encode(sent)\n",
        "            if len(ids) < 2:\n",
        "                continue\n",
        "            input_ids = torch.tensor([ids[:-1]], device=device)\n",
        "            target_ids = torch.tensor([ids[1:]], device=device)\n",
        "            output = model(input_ids)\n",
        "            loss = criterion(output.view(-1, vocab_size), target_ids.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_count += len(ids) - 1\n",
        "    ppl = math.exp(total_loss / total_count)\n",
        "    return ppl"
      ],
      "metadata": {
        "id": "V5Ihxwr83ogR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training on Curriculum\")\n",
        "curr_model = train_model(curriculum_sentences)\n",
        "print(\"Training on Random\")\n",
        "rand_model = train_model(random_sentences)\n",
        "print(\"Training on Child-directed\")\n",
        "child_model = train_model(child_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4yhrmAh3uBG",
        "outputId": "0ad3124c-6d1f-4bda-e75a-b5cdb5d1a123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on Curriculum\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.3911\n",
            "Epoch 2, Loss: 2.5406\n",
            "Training on Random\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.2377\n",
            "Epoch 2, Loss: 4.1454\n",
            "Training on Child-directed\n",
            "Epoch 1, Loss: 3.0184\n",
            "Epoch 2, Loss: 2.9218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "curr_ppl = calculate_perplexity(curr_model, curriculum_sentences[:1000])\n",
        "rand_ppl = calculate_perplexity(rand_model, random_sentences[:1000])\n",
        "child_ppl = calculate_perplexity(child_model, child_sentences[:1000])"
      ],
      "metadata": {
        "id": "p2RUNWwz30Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Curriculum Perplexity: {curr_ppl:.2f}\")\n",
        "print(f\"Random Perplexity: {rand_ppl:.2f}\")\n",
        "print(f\"Child-directed Perplexity: {child_ppl:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBunkCK735VU",
        "outputId": "fb01ec5c-82df-40d4-b1b3-414270d910a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Curriculum Perplexity: 13.47\n",
            "Random Perplexity: 62.64\n",
            "Child-directed Perplexity: 18.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minimal_pairs = [\n",
        "    (\"అమ్మాయిలు బడికి వెళ్తున్నారు.\", \"అమ్మాయిలు బడికి వెళ్తుంది.\"),\n",
        "    (\"అతను పాలు తాగాడు.\", \"అతను పాలు తాగింది.\"),\n",
        "    (\"పిల్లలు ఆడుతున్నారు.\", \"పిల్లలు ఆడుతోంది.\"),\n",
        "    (\"అతడు స్కూల్‌కి వెళ్లాడు.\", \"అతడు స్కూల్‌కి వెళ్తాడు.\"),\n",
        "    (\"అమ్మ బడికి వెళ్లింది.\", \"అమ్మ బడి వెళ్లింది.\"),\n",
        "    (\"ఆమె పుస్తకం చదువుతుంది.\", \"ఆమె పుస్తకం చదువుతాడు.\"),\n",
        "    (\"నేను పాలు తాగాను.\", \"పాలు నేను తాగాను.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "b2Ui1G7d38sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for correct, wrong in minimal_pairs:\n",
        "    c_ids = encode(correct)\n",
        "    w_ids = encode(wrong)\n",
        "    if len(c_ids) < 2 or len(w_ids) < 2:\n",
        "        continue\n",
        "    input_c = torch.tensor([c_ids[:-1]], device=device)\n",
        "    target_c = torch.tensor([c_ids[1:]], device=device)\n",
        "    input_w = torch.tensor([w_ids[:-1]], device=device)\n",
        "    target_w = torch.tensor([w_ids[1:]], device=device)\n",
        "    c_loss = nn.CrossEntropyLoss(ignore_index=0)(curr_model(input_c).view(-1, vocab_size), target_c.view(-1))\n",
        "    w_loss = nn.CrossEntropyLoss(ignore_index=0)(curr_model(input_w).view(-1, vocab_size), target_w.view(-1))\n",
        "    print(f\"Minimal pair: Correct loss={c_loss.item():.4f}, Wrong loss={w_loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdt7Goo54OIy",
        "outputId": "1a8dfef5-84fc-4ef6-899c-7a989e65c77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimal pair: Correct loss=7.2329, Wrong loss=7.9594\n",
            "Minimal pair: Correct loss=4.9048, Wrong loss=4.7248\n",
            "Minimal pair: Correct loss=6.4096, Wrong loss=7.3881\n",
            "Minimal pair: Correct loss=6.8856, Wrong loss=6.9573\n",
            "Minimal pair: Correct loss=7.7698, Wrong loss=8.6271\n",
            "Minimal pair: Correct loss=9.0869, Wrong loss=9.2155\n",
            "Minimal pair: Correct loss=4.7681, Wrong loss=4.9624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sluTkeiZ7VxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}